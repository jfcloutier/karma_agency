# Symbolic cognitive robotics

## Formative concepts
	Active Inference (an agent actively minimizes surprise to survive)
	Enactivism (an agent's perceptions and actions are constructively co-dependent)
	Apperception (sense-making as discovery of unified causal theories)
	Mortal Computing (meaning is grounded in the agent's drive to survive)
	Society of Mind (an agent is animated by a collective of cognition actors interacting with each other and the environment)
	Constraint Closure (the cognition actors constrain how the Society of Mind can change, and vice-versa)
	Kantian Whole (the parts -cognition actors- exist for and by means of the whole -the Society of Mind-)	
	
## Introspection vs extrospection

	The object named "self" is implicitly in the CA's umwelt

	Extrospection => generation of (more or less abstracted) sensations from the external world
	Introspection => generation of cognitive sensations (internal world)
	Detectors and effectors are exposed as extrospective primitive CAs
	Every other CA can be coopted into umwelts as a generator of introspective or extrospective sensations, and as an effector of cognitive actions

## The umwelt of a CA
	A set of other CAs
	What they expose to all other CAs
		the vocabulary of their idiosyncratic beliefs (what others can make predictions about)
			extant, latent and synthetic objects (typed with extant or latent object types), 
			extant, latent and synthetic relations/properties
				a latent or synthetic property is always boolean-valued
		
			all CAs have a common vocabulary of meta-cognition beliefs
	what they emit when prompted by predictions:
		prediction errors from their beliefs
			from perceiving other CAs
			from their cognitive self-assessments/beliefs (from introspection)
		
	Perceiving:
		Discrete time step
			duration is proportional to abstraction level
		Making predictions about umwelt CA beliefs
			and getting prediction errors or not
		
	Perceptions:
		Uncontradicted predictions
		Prediction errors can be emitted in response to prediction, with precision
			If multiple CAs respond to a prediction with prediction errors
				The prediction error with the highest precision is picked
					Tie-breaking is random
		
	The precision of a prediction error (a float between 0 and 1) is a function of:
		The confidence of the emitting CA in the contradicting belief, which is a function of:
			The accuracy of the supporting causal model
			The duration of the supporting trend
				And average precision modulated by the variance in the precision of the perceptions aggregated by the trend
		
## A CA's beliefs - what's imagined, analyzed, partitioned and categorized by the CA - from its perceptions (unrefuted predictions + prediction errors about the beliefs of CAs in its umwelt)

	- Beliefs are available to other CA's as *synthetic or latent* thus *novel* perceptions
	- Beliefs are supported by a causal theory (latent) and by trends in past perceptions (synthetic)
	- Beliefs have associated normativity (pleasant vs unpleasant vs indifferent beliefs)
	
	- Latent - unobserved but imagined/abduced properties/relations/objects to (causally) make sense of observations - the thin now -
	  Synthetic - induced from, and thus supported by, perception trends - the thick now -

	* Trend analysis (trends support the synthesis of beliefs in the thick now)
			
		trend value is either stable, unstable, up or down
	
		Specific trend- trend(<predicate name>(<object name>, <object name> | <domain value>)), <trend value>, <since>) - a trend on an instance of a property/relation (stable, unstable)
		Generic trend - trend(<predicate name>(<object name>)), <trend value>, <since>) - a trends on a type of property/relation for an object (stable, unstable, up, down) - up/down for relations describe counts of related objects, for properties up/down describes rise/fall in values (property value domains are ordered)
		
		A trend and associated normativity can be preserved as long-term memory 
			compressed(<trend>, <time interval>)
			associated with beliefs (their support)
		uncompressed trends represent short-term memory (developing trend)

	* Normativity (from association with current feelings) is always about trends
		feeling(<feeling type>, good | bad | neutral)
			feeling types: hunger, fear, ennui
		trend_value(<trend>, good | bad | neutral)
		a trend takes its (emotional) value from concurrent feelings
		a belief supported by a trend takes the normative value of that trend
			a belief associated with a bad feeling is unpleasant, else it's pleasant (good) or indifferent (neutral)
		since trends have lengths, the normative values of trends have duration - e.g. a long-lasting unpleasant belief are worse than a short-lasting one
		
	* Analyzed: synthetic properties/relations (!= latent) are supported by attention-worthy (strongly felt or surprising) trends
		<synthetic property name>(<object_name>, true | false)
		<synthetic relation name>(<object name>, <object name>)
		
	* Partitioned - in(<new object name>, <object name>) - belief about composition -> object creation
		Parts are induced from detecting boundaries in an observed object.
		How are boundaries detected?
			An object has differentiable, stable sub-trends that coincide in time
			This might indicate that different parts of the object were being observed at different times
				- e.g. "patch of food" in the "ground" in the "environment" in the "world" ("self" is always in the "world")
		A part is usually not of the same object type as the whole (except for fractal objects)
		
	* Categorized: - is_a(<object_name>, <new object_type>) - beliefs about objects having synthetic object types
		Categorization of an object is supported by being believed to be "in" another object
			Synthetic object typing is supported by a coincidence of trends about the parent object
	
	* Significance of a trend
		A trend is significant if it breaks surprisingly from a previous trend, or correlates with a change in feelings
		
## Actions

	Changes in properties/relations observed by a CA are either caused by latent processes or by actions
		In a static environment, they are caused entirely by actions!
			No perception without action and no action without perception
	To make sense of/apperceive the consequences of actions, they must be observed together with the property/relation changes they (may) cause

	A CA exposes by name the actions it can execute
		A CA must always be capable of acting
			i.e. it has at least one effector CA in its transitive umwelt
	
	An intent names an action that a CA wants executed.
		A CA can intend any action in its repertoire
		
	The action repertoire of a CA consists of	
		the actions it synthesized
		plus the distinct actions exposed by CAs in its umwelt
						
	The CA of an effector exposes primitive actions
		A wheel CA exposes the primitive actions "spin" and "reverse spin"
			
	A CA syntesizes actions from the actions exposed by CAs in its umwelt, names them and exposes them in turn
		A synthetic actions is a named list of synthetic actions
			e.g. action_2 = [action(ca_2, action_1), action(ca_2, action_1), action(ca_3, action_2)]
				an action can be repeated
				a synthetic action is, via closure, a sequence of primitive actions
				
	Execution of an intended action is inhibited if another CA concurrently intends an action that 
		covers it (is a super-sequence)
		or is identical and has higher normative motivation
	
	What motivates intents and the synthesis of actions by a CA (from less to most motivated)
		Babbling 
			to maybe cause a "random" belief
		Evidencing 
			to impact confidence in a belief (thus the precsion of reported prediction errors)
		Eliminating 
			to terminate an unpleasant belief
		
	A CA intends at most one motivated action per time slice
		The most motivated action in its repertoire
		A motivation tie is randomly broken
			
	Why does a CA synthesize a new action?
		Because a sequence of actions is empirically associated with a belief change (belief creation, verification, elimination)
			Causation via participation in a causal model that necessitates latent objects/properties/relations (thin now belief)
			Correlation with a belief-supporting trend starting/ending/enduring (long now belief)
				The sequence of actions that runs before/through the trend is extracted
			Babbling (epistemic exploration)
				Make a variation on a synthetic action
					Amplify sub-sequences via action duplication
					Tone down sub-sequences by reducing duplication
					Splice and recombine a synthetic action
				Create from scratch
					Assemble actions from the repertoire
		
	All actions taken are observable by all CAs
		The primitive actions from the closure of synthetic actions are observed
		During time slice T of the CA
			If a sub-sequence of the observed primitive actions recreates a synthetic action in the repertoire of the CA
				then the longest synthetic action is what is observed, plus the second longest etc.
		
	A policy is an action associated with a belief, a goal (verification, elimination) and a success rating

## Feelings

	Feelings are agent-wide signals about detected existential risks

	Feeling types
		Hunger
			Energy/resource depletion
		Pain
			Loss of structural integrity
		Fear
			Inability to predict

	Motivational ranking
		Hunger > Pain > Fear
		
	The agent dies when energy/resources are depleted

	The agent is immobilized when pain is too high

	Feelings are centrally computed from
		detector sensations
			touch - pain increases
			color - resources increase if color == food type
		effector sensations
			work done - energy decreases
		CA cognitive sensations
			mental effort - energy decreases
			prediction success rate - fear increases/decreases
			relevance (rate of received predictions, intended composited actions)
		The passing of time 
			healing - pain decreases
			base metabolism - resources/energy decreases

	Any change in hunger/pain/fear intensity is signaled to all CAs

	For each CA
		In each time slice, there's an average intensity of each feeling type

## Constraints
 
	Umwelts (when closed) must be acyclic directed graphs but not necessarily trees
	
	Abstraction must be monotonic
		A CA must not include a CA in its immediate umwelt if the latter is already in its transitive umwelt.
	
	A CA must be either cognitive or meta-cognitive, never both
	
	Only one synthetic action in a conflicting set can be executing at any given point in time
		A synthetic action conflicts with another if their closed sequences have any simple action type in common.
		Practically speaking, only one synthetic action is allowed to execute at any time

	A CA must not remove an element from its API if it is used by another CA
		to formulate a causal theory
		to synthesize a belief or action

	If a CA must archive a belief (without normativity) or action and its (compressed - abstracted) support when the support is gone but the belief or action is still used by other CAs

	A new belief must not be created if its support is subsumed by the compressed support of an archived belief
		The archive belief is ressucitated and given the current support

	An archived belief/action must be deleted if the belief/action is no longer used by another CA.
	
	

